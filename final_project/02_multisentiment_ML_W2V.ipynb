{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multisentiment_ML_W2V",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+5sBLL5dwtEV4pMfaYEy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwanglo/mge51101-20195171/blob/master/final_project/02_multisentiment_ML_W2V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGjq3cn26J8f",
        "colab_type": "text"
      },
      "source": [
        "# **Multi-sentiment classification using machine learning**\n",
        "\n",
        "In this section, we will build multi-sentiment classification models using machine learning techniques. <br>\n",
        "<br>\n",
        "**Applied embedding :** fastText Korean ver. using wikipedia<br>\n",
        "Applied vectorizer : <br>\n",
        "Count Vector, TF-IDF, Ngram, Character level TF-IDF\n",
        "<br>\n",
        "\n",
        "**Applied machine learning model :** <br>\n",
        "Naive Bayes, Logistic Regression, Support Vector Machine, Random Forest\n",
        "<br>\n",
        "\n",
        "**Reference** <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-w7T_1u7ob1",
        "colab_type": "text"
      },
      "source": [
        "1. Link to google drive\n",
        "2. Import required libraries\n",
        "3. Load prepared dataset\n",
        "4. Divide into train, valid, test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkchLDVyGZT2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "501ef413-b522-4499-862d-7410fb59f039"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSJ9GlOVG_4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install konlpy\n",
        "!pip3 install soynlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa_YsxZ8G3sC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d46bbe9-57ef-4892-f847-43e9a663fbfe"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import xgboost, string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olBIKYKNGtDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/gdrive/My Drive/Colab Notebooks/Final Project/dataset/'\n",
        "\n",
        "train_data = pd.read_csv(path+\"sentiment_train.csv\")\n",
        "valid_data = pd.read_csv(path+\"sentiment_valid.csv\")\n",
        "test_data = pd.read_csv(path+\"sentiment_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyUnYjE9HHSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "db954a5b-93d6-437d-c7ff-c026a6613b30"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>약은 최대한 안먹으려고 하는데좋은 음시있나요?0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>몸무게 1키로찌는건 아니겠죠?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>보통 가진통도 이렇게 오래가나요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>여자가 술취해서 먼저 전화하는거 짜증나요???</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>아무래도 무리겠죠?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Sentence  Emotion\n",
              "0  약은 최대한 안먹으려고 하는데좋은 음시있나요?0        1\n",
              "1            몸무게 1키로찌는건 아니겠죠?        1\n",
              "2          보통 가진통도 이렇게 오래가나요?        1\n",
              "3   여자가 술취해서 먼저 전화하는거 짜증나요???        1\n",
              "4                  아무래도 무리겠죠?        1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYZ37bn3HV-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, train_y = train_data['Sentence'], train_data['Emotion']\n",
        "valid_x, valid_y = valid_data['Sentence'], valid_data['Emotion']\n",
        "test_x, test_y = test_data['Sentence'], test_data['Emotion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qxtnShL8DCP",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing before training\n",
        "\n",
        "1. Import vectorizer\n",
        "2. Set stopwords and build clean dataset\n",
        "3. Tokenize dataset\n",
        "4. Import pre-trained embedding from fastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdeXglYsHYpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(train_data['Sentence'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)\n",
        "xtest_count =  count_vect.transform(test_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTsp4iw8Hccs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "30bd0701-7937-4aa7-e8df-adb3ddc49582"
      },
      "source": [
        "#TF-IDF vectorizer\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=15000)\n",
        "tfidf_vect.fit(train_data['Sentence'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "xtest_tfidf =  tfidf_vect.transform(test_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=15000)\n",
        "tfidf_vect_ngram.fit(train_data['Sentence'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=15000)\n",
        "tfidf_vect_ngram_chars.fit(train_data['Sentence'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)\n",
        "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLWQ2C8Ld7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words_set = pd.read_csv(path+'stopwords100.txt',header = 0, delimiter = '\\t', quoting = 3)\n",
        "stop_words= (list(stop_words_set['aa']))\n",
        "stop_words2 = ['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한']\n",
        "stop_words.extend(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNg66v0_L0nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "from soynlp.normalizer import *\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "def preprocessing(text, okt, remove_stopwords = False, stop_words = []):\n",
        "    text = only_hangle(text)\n",
        "    text = repeat_normalize(text, num_repeats = 2)\n",
        "    \n",
        "    text_token = okt.morphs(text, stem = True)\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        text_token = [token for token in text_token if not token in stop_words]\n",
        "        \n",
        "    return text_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPxya28gL5Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing - Train and Valid data\n",
        "okt = Okt()\n",
        "clean_train_data = []\n",
        "clean_valid_data = []\n",
        "clean_test_data = []\n",
        "for text in train_data['Sentence']:\n",
        "    if type(text) == str:\n",
        "        clean_train_data.append(preprocessing(text, okt, remove_stopwords = True, stop_words = stop_words))\n",
        "    else:\n",
        "        clean_train_data.append([])\n",
        "        \n",
        "for text in valid_data['Sentence']:\n",
        "    if type(text) == str:\n",
        "        clean_valid_data.append(preprocessing(text, okt, remove_stopwords = True, stop_words = stop_words))\n",
        "    else:\n",
        "        clean_valid_data.append([])\n",
        "        \n",
        "for text in test_data['Sentence']:\n",
        "    if type(text) == str:\n",
        "        clean_test_data.append(preprocessing(text, okt, remove_stopwords = True, stop_words = stop_words))\n",
        "    else:\n",
        "        clean_test_data.append([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5stG-yqQCug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_data)\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_data)\n",
        "valid_sequences = tokenizer.texts_to_sequences(clean_valid_data)\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_data)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aha_MU0XQGbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "for i, line in enumerate(open(path+'wiki.ko.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "    \n",
        "    \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-pVzZAI8xRr",
        "colab_type": "text"
      },
      "source": [
        "# Model building\n",
        "\n",
        "1. Set train and test models\n",
        "2. Get machine learning models from scikit learn\n",
        "3. Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBp9ZWfDQTQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 기존 모델\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XM37oHXauiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From other reference\n",
        "def test_model(classifier, X_train, y_train, X_test):\n",
        "  \n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "\n",
        "  return metrics.accuracy_score(y_pred, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyMAeLC8SDle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAM7AjzlY-HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hl99HgXRzf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e49f02d2-4128-445f-ebf6-a1ad31011503"
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
        "\n",
        "print(\"NB, Count Vectors Accuracy\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors Accuracy\n",
            "Validation Accuracy:  0.38013571869216534\n",
            "Test Accuracy:  0.37424425634824665\n",
            "NB, WordLevel TF-IDF\n",
            "Validation Accuracy:  0.36964836520666255\n",
            "Test Accuracy:  0.3653480739333218\n",
            "NB, N-Gram Vectors\n",
            "Validation Accuracy:  0.16681061073411474\n",
            "Test Accuracy:  0.1731732596303334\n",
            "NB, CharLevel Vectors\n",
            "Validation Accuracy:  0.48834053053670573\n",
            "Test Accuracy:  0.4920538953187079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzqABsj49BUZ",
        "colab_type": "text"
      },
      "source": [
        "After training, f1-score and confusion matrix was tested as mentioned in proposal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZdMBxyyk_D9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f_scores(classifier, X_train, y_train, X_test):\n",
        "  \n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA9U_s87Xzwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84e2cade-e46a-4d4f-bbad-b45b2e647033"
      },
      "source": [
        "#F1-score \n",
        "y_pred = f_scores(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
        "\n",
        "precision_recall_fscore_support(test_y, y_pred, average='weighted')\n",
        "#Precision / Recall / F1_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.481089554780168, 0.4920538953187079, 0.47989521354227416, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJH5JbaRkDce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7cdbfe84-b510-4939-c341-3b77b3e4b1d0"
      },
      "source": [
        "#Confusion Matrix\n",
        "confusion_matrix(test_y, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 230,  158,  328,  234,   99,  166,  234],\n",
              "       [  70,  918,  210,   50,  261,   71,   60],\n",
              "       [  92,  215,  989,  115,  115,  133,  110],\n",
              "       [  59,   89,  226,  854,   77,   45,  350],\n",
              "       [  36,  284,  115,   69,  943,   81,   52],\n",
              "       [  60,   56,  201,   51,  105, 1287,   51],\n",
              "       [ 119,   94,  257,  526,   89,   68,  476]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wRDvjGNSJXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6aad425-6574-4942-81b4-0f69b8856fbb"
      },
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
        "print(\"LR, Count Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf)\n",
        "print(\"NB, CharLevel Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
        "\n",
        "print(\"LR, N-Gram Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors\n",
            "Validation Accuracy:  0.3731030228254164\n",
            "Test Accuracy:  0.3667300051822422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NB, CharLevel Vectors\n",
            "Validation Accuracy:  0.36557680444170265\n",
            "Test Accuracy:  0.35792019347037485\n",
            "LR, N-Gram Vectors\n",
            "Validation Accuracy:  0.16693399136335596\n",
            "Test Accuracy:  0.17256866470893073\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, CharLevel Vectors\n",
            "Validation Accuracy:  0.4983343615052437\n",
            "Test Accuracy:  0.4931767144584557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq5lQmgLT1kG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "58f70f9f-f510-4065-8b19-0bdf620e292f"
      },
      "source": [
        "#F1-score \n",
        "y_pred = f_scores(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
        "\n",
        "precision_recall_fscore_support(test_y, y_pred, average='weighted')\n",
        "#Precision / Recall / F1_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4896338402343924, 0.4931767144584557, 0.4904485878130712, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaQcMs9ZT1xz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "73a1a186-9f7a-4f60-e100-04d919367040"
      },
      "source": [
        "#Confusion Matrix\n",
        "confusion_matrix(test_y, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 376,  103,  266,  221,   78,  160,  245],\n",
              "       [ 115,  859,  218,   51,  256,   62,   79],\n",
              "       [ 167,  170,  937,  108,  106,  129,  152],\n",
              "       [ 128,   66,  183,  776,   63,   44,  440],\n",
              "       [  70,  249,  115,   75,  923,   84,   64],\n",
              "       [ 105,   44,  157,   52,   92, 1292,   69],\n",
              "       [ 210,   72,  202,  452,   78,   68,  547]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGuMm2m7R1RX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6718375c-67ec-438b-e436-23ba5f85b305"
      },
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "valid_accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "test_accuracy = test_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM, N-Gram Vectors\n",
            "Validation Accuracy:  0.16594694632942628\n",
            "Test Accuracy:  0.17127310416306787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIXXtfOzTCBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4cd153c4-0c34-4dee-c7cd-f8a3458762d3"
      },
      "source": [
        "# RF on Count Vectors\n",
        "valid_accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "test_accuracy = test_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xtest_count)\n",
        "print(\"RF, Count Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "valid_accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "test_accuracy = test_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xtest_tfidf)\n",
        "print( \"RF, WordLevel TF-IDF\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF, Count Vectors\n",
            "Validation Accuracy:  0.30475015422578655\n",
            "Test Accuracy:  0.31188460874071516\n",
            "RF, WordLevel TF-IDF\n",
            "Validation Accuracy:  0.3230104873534855\n",
            "Test Accuracy:  0.32276731732596303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpHoMyF9QVa",
        "colab_type": "text"
      },
      "source": [
        "By reviewing all results, Naive Bayes and Logistic Regression using Char-level TF-IDF vectorizer was selected as top 2 accuracy and f1-score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ThpduLDTEgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}