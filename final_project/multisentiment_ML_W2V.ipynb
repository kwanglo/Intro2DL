{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multisentiment_ML_W2V",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8OQVkocskkVaKlFTCQ9xf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwanglo/mge51101-20195171/blob/master/final_project/multisentiment_ML_W2V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkchLDVyGZT2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "033da25b-bdcf-4ae5-9a7b-a7fb3c02a214"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSJ9GlOVG_4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install konlpy\n",
        "!pip3 install soynlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa_YsxZ8G3sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import xgboost, string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olBIKYKNGtDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/gdrive/My Drive/Colab Notebooks/Final Project/dataset/'\n",
        "\n",
        "train_data = pd.read_csv(path+\"sentiment_train.csv\")\n",
        "valid_data = pd.read_csv(path+\"sentiment_valid.csv\")\n",
        "test_data = pd.read_csv(path+\"sentiment_test.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyUnYjE9HHSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYZ37bn3HV-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, train_y = train_data['Sentence'], train_data['Emotion']\n",
        "valid_x, valid_y = valid_data['Sentence'], valid_data['Emotion']\n",
        "test_x, test_y = test_data['Sentence'], test_data['Emotion']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdeXglYsHYpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(train_data['Sentence'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)\n",
        "xtest_count =  count_vect.transform(test_x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTsp4iw8Hccs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e0fb1443-1f90-469e-a735-d67eee73762d"
      },
      "source": [
        "#TF-IDF vectorizer\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=15000)\n",
        "tfidf_vect.fit(train_data['Sentence'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "xtest_tfidf =  tfidf_vect.transform(test_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=15000)\n",
        "tfidf_vect_ngram.fit(train_data['Sentence'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=15000)\n",
        "tfidf_vect_ngram_chars.fit(train_data['Sentence'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)\n",
        "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLWQ2C8Ld7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words_set = pd.read_csv(path+'stopwords100.txt',header = 0, delimiter = '\\t', quoting = 3)\n",
        "stop_words= (list(stop_words_set['aa']))\n",
        "stop_words2 = ['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한']\n",
        "stop_words.extend(stop_words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNg66v0_L0nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "from soynlp.normalizer import *\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "def preprocessing(text, okt, remove_stopwords = False, stop_words = []):\n",
        "    text = only_hangle(text)\n",
        "    text = repeat_normalize(text, num_repeats = 2)\n",
        "    \n",
        "    text_token = okt.morphs(text, stem = True)\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        text_token = [token for token in text_token if not token in stop_words]\n",
        "        \n",
        "    return text_token"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPxya28gL5Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing - Train and Valid data\n",
        "okt = Okt()\n",
        "clean_train_data = []\n",
        "clean_valid_data = []\n",
        "clean_test_data = []\n",
        "for text in train_data['Sentence']:\n",
        "    if type(text) == str:\n",
        "        clean_train_data.append(preprocessing(text, okt, remove_stopwords = True, stop_words = stop_words))\n",
        "    else:\n",
        "        clean_train_data.append([])\n",
        "        \n",
        "for text in valid_data['Sentence']:\n",
        "    if type(text) == str:\n",
        "        clean_valid_data.append(preprocessing(text, okt, remove_stopwords = True, stop_words = stop_words))\n",
        "    else:\n",
        "        clean_valid_data.append([])\n",
        "        \n",
        "for text in test_data['Sentence']:\n",
        "    if type(text) == str:\n",
        "        clean_test_data.append(preprocessing(text, okt, remove_stopwords = True, stop_words = stop_words))\n",
        "    else:\n",
        "        clean_test_data.append([])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cErsgfpOooO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "4c7ceb80-6315-481f-86bd-3dac7845533b"
      },
      "source": [
        "clean_train_data[:4]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['대본', '없다', '도', '어버버', '하다', '걸리다', '닭'],\n",
              " ['거', '믿다', '진실', '은', '결국', '은', '거짓', '선동', '인가'],\n",
              " ['그러다가',\n",
              "  '제대',\n",
              "  '하다',\n",
              "  '헤어지다',\n",
              "  '이렇다',\n",
              "  '은',\n",
              "  '그냥',\n",
              "  '휴가',\n",
              "  '나오다',\n",
              "  '한번',\n",
              "  '하다',\n",
              "  '이렇다',\n",
              "  '용도',\n",
              "  '라',\n",
              "  '던데'],\n",
              " ['어제', '런닝맨', '완전', '재밌다']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5stG-yqQCug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_data)\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_data)\n",
        "valid_sequences = tokenizer.texts_to_sequences(clean_valid_data)\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_data)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 70"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aha_MU0XQGbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "for i, line in enumerate(open(path+'wiki.ko.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "    \n",
        "    \n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBp9ZWfDQTQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 기존 모델\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XM37oHXauiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From other reference\n",
        "def test_model(classifier, X_train, y_train, X_test):\n",
        "  \n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "\n",
        "  return metrics.accuracy_score(y_pred, test_y)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyMAeLC8SDle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAM7AjzlY-HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hl99HgXRzf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d023b120-055d-440e-978e-1ccf100fe514"
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
        "\n",
        "print(\"NB, Count Vectors Accuracy\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "valid_accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "test_accuracy = test_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors Accuracy\n",
            "Validation Accuracy:  0.38013571869216534\n",
            "Test Accuracy:  0.37424425634824665\n",
            "NB, WordLevel TF-IDF\n",
            "Validation Accuracy:  0.36964836520666255\n",
            "Test Accuracy:  0.3653480739333218\n",
            "NB, N-Gram Vectors\n",
            "Validation Accuracy:  0.16681061073411474\n",
            "Test Accuracy:  0.1731732596303334\n",
            "NB, CharLevel Vectors\n",
            "Validation Accuracy:  0.48834053053670573\n",
            "Test Accuracy:  0.4920538953187079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA9U_s87Xzwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wRDvjGNSJXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4402cccc-544c-415c-d3c5-8b5adfbe15d8"
      },
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
        "print(\"LR, Count Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf)\n",
        "print(\"NB, CharLevel Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
        "\n",
        "print(\"LR, N-Gram Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "valid_accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "test_accuracy = test_model(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
        "print(\"LR, CharLevel Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors\n",
            "Validation Accuracy:  0.3731030228254164\n",
            "Test Accuracy:  0.3667300051822422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NB, CharLevel Vectors\n",
            "Validation Accuracy:  0.36557680444170265\n",
            "Test Accuracy:  0.35792019347037485\n",
            "LR, N-Gram Vectors\n",
            "Validation Accuracy:  0.16693399136335596\n",
            "Test Accuracy:  0.17256866470893073\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, CharLevel Vectors\n",
            "Validation Accuracy:  0.3731030228254164\n",
            "Test Accuracy:  0.3667300051822422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGuMm2m7R1RX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f607bd22-2162-452a-b1c2-e3520b189b2b"
      },
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "valid_accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "test_accuracy = test_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM, N-Gram Vectors\n",
            "Validation Accuracy:  0.16594694632942628\n",
            "Test Accuracy:  0.17127310416306787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIXXtfOzTCBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "296cac8f-e29d-46a4-847c-5e1474c9d2d4"
      },
      "source": [
        "# RF on Count Vectors\n",
        "valid_accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "test_accuracy = test_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xtest_count)\n",
        "print(\"RF, Count Vectors\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "valid_accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "test_accuracy = test_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xtest_tfidf)\n",
        "print( \"RF, WordLevel TF-IDF\")\n",
        "print(\"Validation Accuracy: \", valid_accuracy)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF, Count Vectors\n",
            "Validation Accuracy:  0.3068476249228871\n",
            "Test Accuracy:  0.3117982380376576\n",
            "RF, WordLevel TF-IDF\n",
            "Validation Accuracy:  0.32387415175817397\n",
            "Test Accuracy:  0.3186215235792019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ThpduLDTEgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}