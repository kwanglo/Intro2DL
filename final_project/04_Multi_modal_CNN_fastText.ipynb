{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "combined_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvvK3kwKr7LZ9ujkC6Pfur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwanglo/mge51101-20195171/blob/master/final_project/04_Multi_modal_CNN_fastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0HSbLNohKgI",
        "colab_type": "text"
      },
      "source": [
        "# **Multi-modal Analysis using CNN+FastText Embedding**\n",
        "\n",
        "In this section, we will build multi-modal classifier for both sentiment and utterance using CNN+FastText Embedding <br>\n",
        "<br>\n",
        "**Applied embedding :** <br>\n",
        "fastText Korean ver. using wikipedia<br>\n",
        "**Applied deep learning model :** <br>\n",
        "CNN\n",
        "<br>\n",
        "\n",
        "**Reference** <br>\n",
        "Code used in current page refered to below link.\n",
        "\n",
        "1. https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb\n",
        "2. https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
        "3. https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/5%20-%20Multi-class%20Sentiment%20Analysis.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv8ZMQS9dr5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "b24ae33a-5944-4108-84b3-fc5a07b9f8ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWBmpvaLd9Nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1742d2fa-b276-4bfd-a1e1-b61097bf75a3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 19 18:28:53 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plvd7H_seCML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "94464a4f-d817-41c5-8bbb-7e075d65fcb6"
      },
      "source": [
        "!pip3 install konlpy\n",
        "!pip3 install soynlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 51.7MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/9b/e115101a833605b3c0e6f3a2bc1f285c95aaa1d93ab808314ca1bde63eed/JPype1-0.7.5-cp36-cp36m-manylinux2010_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Installing collected packages: JPype1, colorama, tweepy, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-0.7.5 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n",
            "Collecting soynlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->soynlp) (0.15.1)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOl46e9beMHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "from sklearn import datasets, model_selection\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO5Vo2CxeQPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/gdrive/My Drive/Colab Notebooks/Final Project/dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD4Muf7YhA1i",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing - Emotion\n",
        "\n",
        "Since we already done separating dataset, we can jump to data preprocessing session. <br>\n",
        "We will implement KoNLPy Okt tokenizer and stopwords to refine dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oVN6ccTg1VK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "from soynlp.normalizer import *\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "def tokenizer(text): # create a tokenizer function\n",
        "    okt = Okt()\n",
        "    text = only_hangle(text)\n",
        "    text = repeat_normalize(text, num_repeats = 2)\n",
        "    x = okt.morphs(text , stem= True)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOw3Ww5nhGID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words_set = pd.read_csv(path+'stopwords100.txt',header = 0, delimiter = '\\t', quoting = 3)\n",
        "stop_words= (list(stop_words_set['aa']))\n",
        "stop_words2 = ['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한']\n",
        "stop_words.extend(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjtA7_CbijNC",
        "colab_type": "text"
      },
      "source": [
        "Now we will build input value TEXT and LABEL for torch.text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMbhpZmuhIRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "SEED = 3432\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "TEXT_emo = data.Field(tokenize=tokenizer, stop_words = stop_words)\n",
        "LABEL_emo = data.LabelField()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6722Q_kYG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R99Oem4k0St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset\n",
        "fields_emo = [(\"Sentence\", TEXT_emo),(\"Emotion\", LABEL_emo)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HokxYlu_k3oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_emo,valid_emo, test_emo = data.TabularDataset.splits(\n",
        "                                        path = path,\n",
        "                                        train = 'sentiment_train.csv',\n",
        "                                        validation = 'sentiment_valid.csv',\n",
        "                                        test = 'sentiment_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields_emo,\n",
        "                                        skip_header = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-suCA51k8Gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "655294ac-bf40-4b2b-bf2e-aeefa86be3ac"
      },
      "source": [
        "vars(train_emo[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Emotion': '5', 'Sentence': ['어제', '런닝맨', '완전', '재밌다']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fwAWwx1jDej",
        "colab_type": "text"
      },
      "source": [
        "This will import FastText word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpV_SIrfllGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "vec = torchtext.vocab.Vectors('wiki.ko.vec', cache=path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz6r-BUelogp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT_emo.build_vocab(train_emo, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = vec, \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL_emo.build_vocab(train_emo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EEHQvW8lunB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Iterator, BucketIterator\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator_emo, valid_iterator_emo, test_iterator_emo = data.BucketIterator.splits(\n",
        "    (train_emo, valid_emo, test_emo), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device, sort = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ0BXF_KmDk0",
        "colab_type": "text"
      },
      "source": [
        "# **Data preprocessing - Utterance**\n",
        "\n",
        "Utterance follows same preprocessing procedure as sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qczuf5A64CbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 3432\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT_utt = data.Field(tokenize=tokenizer, stop_words = stop_words)\n",
        "LABEL_utt = data.LabelField()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0wp4Q6Dl8nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fields_utt = [(\"text\", TEXT_utt),(\"label\", LABEL_utt)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41qPXuw0maM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_utt,valid_utt, test_utt = data.TabularDataset.splits(\n",
        "                                        path = path,\n",
        "                                        train = 'utterance_train.csv',\n",
        "                                        validation = 'utterance_valid.csv',\n",
        "                                        test = 'utterance_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields_utt,\n",
        "                                        skip_header = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTRN7TW2mvMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT_utt.build_vocab(train_utt, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = vec, \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL_utt.build_vocab(train_utt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNClEjcUwXg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator_utt, valid_iterator_utt, test_iterator_utt = data.BucketIterator.splits(\n",
        "    (train_utt, valid_utt, test_utt), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device, sort = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYsJjjn6nE_j",
        "colab_type": "text"
      },
      "source": [
        "#**Model building**\n",
        "\n",
        "Identical CNN model was applied for training. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyXp6hyjm9ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_emo(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        text = text.permute(1, 0)        \n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoBHi_3pnJkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_utt(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        text = text.permute(1, 0)        \n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKUzNV6AnQPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM_emo = len(TEXT_emo.vocab)\n",
        "OUTPUT_DIM_emo = len(LABEL_emo.vocab)\n",
        "PAD_IDX_emo = TEXT_emo.vocab.stoi[TEXT_emo.pad_token]\n",
        "\n",
        "INPUT_DIM_utt = len(TEXT_utt.vocab)\n",
        "OUTPUT_DIM_utt = len(LABEL_utt.vocab)\n",
        "PAD_IDX_utt = TEXT_utt.vocab.stoi[TEXT_utt.pad_token]\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [2,3,4]\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model_emo = CNN_emo(INPUT_DIM_emo, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM_emo, DROPOUT, PAD_IDX_emo)\n",
        "model_utt = CNN_utt(INPUT_DIM_utt, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM_utt, DROPOUT, PAD_IDX_utt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfCARoBHk36T",
        "colab_type": "text"
      },
      "source": [
        "Before you move further, you need to check the count parameters fit to your expectations. If they are too large or small, there might need some adjustment in preprocessing or TEXT,LABEL field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5HzKE_pn9L8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ef70b8f9-9272-4265-8a16-050686a56316"
      },
      "source": [
        "def count_parameters_emo(model_emo):\n",
        "    return sum(p.numel() for p in model_emo.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters_emo(model_emo):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 5,058,307 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gku5cb9BoFOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e442be87-c87f-4162-c89e-3cc999f16a3f"
      },
      "source": [
        "def count_parameters_utt(model_utt):\n",
        "    return sum(p.numel() for p in model_utt.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters_utt(model_utt):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,218,707 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5hkGTZHoQ5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_embeddings_emo = TEXT_emo.vocab.vectors\n",
        "pretrained_embeddings_utt = TEXT_utt.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJx4PmC8oJrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNK_IDX_emo = TEXT_emo.vocab.stoi[TEXT_emo.unk_token]\n",
        "UNK_IDX_utt = TEXT_utt.vocab.stoi[TEXT_utt.unk_token]\n",
        "\n",
        "model_emo.embedding.weight.data[UNK_IDX_emo] = torch.zeros(EMBEDDING_DIM)\n",
        "model_emo.embedding.weight.data[PAD_IDX_emo] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "model_utt.embedding.weight.data[UNK_IDX_utt] = torch.zeros(EMBEDDING_DIM)\n",
        "model_utt.embedding.weight.data[PAD_IDX_utt] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbpHXbPPo5bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer_emo = optim.Adam(model_emo.parameters())\n",
        "optimizer_utt = optim.Adam(model_utt.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_emo = model_emo.to(device)\n",
        "model_utt = model_utt.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0caTfGaLpEUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SkGvFAb4PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.metrics as sk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqq8FmealN8W",
        "colab_type": "text"
      },
      "source": [
        "Below are train & validation and test models for each task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLMksDs0pO2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_emo(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.Sentence)\n",
        "        \n",
        "        loss = criterion(predictions, batch.Emotion)\n",
        "        \n",
        "        acc = categorical_accuracy(predictions, batch.Emotion)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mykuXo51pTZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_emo(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    y_pred = []\n",
        "    y_actual = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.Sentence)\n",
        "            pred = torch.max(predictions, 1).indices\n",
        "\n",
        "            pred = pred.tolist()\n",
        "            pred\n",
        "            actual = batch.Emotion.tolist()\n",
        "            actual\n",
        "\n",
        "            loss = criterion(predictions, batch.Emotion)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.Emotion)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            y_pred = y_pred + pred\n",
        "            y_actual = y_actual + actual\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), y_pred, y_actual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31SzjxTbtar3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_utt(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text)        \n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3__scl4ctakR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_utt(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    y_pred = []\n",
        "    y_actual = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text)\n",
        "            pred = torch.max(predictions, 1).indices\n",
        "\n",
        "            pred = pred.tolist()\n",
        "            pred\n",
        "            actual = batch.label.tolist()\n",
        "            actual\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            y_pred = y_pred + pred\n",
        "            y_actual = y_actual + actual\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), y_pred, y_actual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqcEGa3MpUh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2XsOLCqpXTb",
        "colab_type": "text"
      },
      "source": [
        "# **Model training**\n",
        "\n",
        "torch.save(model_emo.state_dict(), 'emo-model.pt') will save best parameters for validation and recalled afterward. This prevents overfitting issue when evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMeltC8PpVru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "9457b0cb-794f-4e98-96d5-11c3b7cd4f4a"
      },
      "source": [
        "#Emotion\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss_emo, train_acc_emo = train_model_emo(model_emo, train_iterator_emo, optimizer_emo, criterion)\n",
        "    valid_loss_emo, valid_acc_emo, y_predict, y_real = evaluate_model_emo(model_emo, valid_iterator_emo, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss_emo < best_valid_loss:\n",
        "        best_valid_loss = valid_loss_emo\n",
        "        torch.save(model_emo.state_dict(), 'emo-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss_emo:.3f} | Train Acc: {train_acc_emo*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss_emo:.3f} |  Val. Acc: {valid_acc_emo*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 1.801 | Train Acc: 29.77%\n",
            "\t Val. Loss: 1.595 |  Val. Acc: 37.67%\n",
            "Epoch: 02 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 1.491 | Train Acc: 43.44%\n",
            "\t Val. Loss: 1.546 |  Val. Acc: 39.53%\n",
            "Epoch: 03 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 1.296 | Train Acc: 51.38%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 41.52%\n",
            "Epoch: 04 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 1.106 | Train Acc: 59.22%\n",
            "\t Val. Loss: 1.596 |  Val. Acc: 41.85%\n",
            "Epoch: 05 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.919 | Train Acc: 66.92%\n",
            "\t Val. Loss: 1.714 |  Val. Acc: 42.22%\n",
            "Epoch: 06 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.752 | Train Acc: 72.82%\n",
            "\t Val. Loss: 1.821 |  Val. Acc: 42.14%\n",
            "Epoch: 07 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.621 | Train Acc: 77.42%\n",
            "\t Val. Loss: 1.945 |  Val. Acc: 42.48%\n",
            "Epoch: 08 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.508 | Train Acc: 81.98%\n",
            "\t Val. Loss: 2.121 |  Val. Acc: 42.28%\n",
            "Epoch: 09 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.443 | Train Acc: 84.59%\n",
            "\t Val. Loss: 2.315 |  Val. Acc: 42.18%\n",
            "Epoch: 10 | Epoch Time: 0m 23s\n",
            "\tTrain Loss: 0.364 | Train Acc: 87.28%\n",
            "\t Val. Loss: 2.497 |  Val. Acc: 41.54%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQIj6z35rXxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "c6f2ac5a-61fc-4fa3-f387-40cb26e0fe4e"
      },
      "source": [
        "model_emo.load_state_dict(torch.load('emo-model.pt'))\n",
        "\n",
        "test_loss_emo, test_acc_emo, pred, actual = evaluate_model_emo(model_emo, test_iterator_emo, criterion)\n",
        "\n",
        "f1_score = sk.f1_score(pred,pred, average = 'weighted')\n",
        "print(f'Test Loss: {test_loss_emo:.3f} | Test Acc: {test_acc_emo*100:.2f}% | F1 Score: {f1_score:.2f}')\n",
        "confusion_matrix(actual, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.533 | Test Acc: 41.14% | F1 Score: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 897,  114,   71,  205,  262,  142,   78],\n",
              "       [ 296, 1088,   29,   79,  157,  107,   55],\n",
              "       [ 279,   34,  357,  106,  778,   87,   59],\n",
              "       [ 346,   49,   42,  679,  159,  309,   56],\n",
              "       [ 334,   52,  196,  102,  785,   86,   74],\n",
              "       [ 207,   64,   36,  247,  153,  837,   36],\n",
              "       [ 423,  122,  100,  175,  406,  105,  118]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkrRGaZ9r_b6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "90fe755d-9158-45b1-c302-8e63b46196ac"
      },
      "source": [
        "#Utterance\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss_utt, train_acc_utt = train_model_utt(model_utt, train_iterator_utt, optimizer_utt, criterion)\n",
        "    valid_loss_utt, valid_acc_utt, pred, actual = evaluate_model_utt(model_utt, valid_iterator_utt, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss_utt < best_valid_loss:\n",
        "        best_valid_loss = valid_loss_utt\n",
        "        torch.save(model_utt.state_dict(), 'utt-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss_utt:.3f} | Train Acc: {train_acc_utt*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss_utt:.3f} |  Val. Acc: {valid_acc_utt*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 1.240 | Train Acc: 53.87%\n",
            "\t Val. Loss: 1.060 |  Val. Acc: 61.45%\n",
            "Epoch: 02 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.980 | Train Acc: 64.58%\n",
            "\t Val. Loss: 0.997 |  Val. Acc: 64.56%\n",
            "Epoch: 03 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.839 | Train Acc: 69.44%\n",
            "\t Val. Loss: 1.022 |  Val. Acc: 63.84%\n",
            "Epoch: 04 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.729 | Train Acc: 73.33%\n",
            "\t Val. Loss: 1.041 |  Val. Acc: 65.57%\n",
            "Epoch: 05 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.638 | Train Acc: 76.19%\n",
            "\t Val. Loss: 1.081 |  Val. Acc: 65.24%\n",
            "Epoch: 06 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.567 | Train Acc: 79.07%\n",
            "\t Val. Loss: 1.146 |  Val. Acc: 65.25%\n",
            "Epoch: 07 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.514 | Train Acc: 80.61%\n",
            "\t Val. Loss: 1.213 |  Val. Acc: 64.47%\n",
            "Epoch: 08 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.464 | Train Acc: 82.71%\n",
            "\t Val. Loss: 1.278 |  Val. Acc: 64.13%\n",
            "Epoch: 09 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.417 | Train Acc: 84.33%\n",
            "\t Val. Loss: 1.354 |  Val. Acc: 64.57%\n",
            "Epoch: 10 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.388 | Train Acc: 85.25%\n",
            "\t Val. Loss: 1.468 |  Val. Acc: 64.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LUjWStksw9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "8e9a06d0-7dc9-4c02-91ea-f27e011233a8"
      },
      "source": [
        "model_utt.load_state_dict(torch.load('utt-model.pt'))\n",
        "\n",
        "test_loss_utt, test_acc_utt, pred, actual = evaluate_model_utt(model_utt, test_iterator_utt, criterion)\n",
        "\n",
        "f1_score = sk.f1_score(pred,pred, average = 'weighted')\n",
        "print(f'Test Loss: {test_loss_emo:.3f} | Test Acc: {test_acc_emo*100:.2f}% | F1 Score: {f1_score:.2f}')\n",
        "confusion_matrix(actual, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.533 | Test Acc: 41.14% | F1 Score: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4330,  487,  394,  293,   37,   12,   27],\n",
              "       [ 903, 3590,  699,   52,   34,   31,   13],\n",
              "       [1036,  890, 1851,   52,    9,    7,   14],\n",
              "       [ 194,   26,   29, 1521,    4,    1,    5],\n",
              "       [ 391,   97,   71,   41,  356,    2,   10],\n",
              "       [ 314,   59,   27,   24,   16,   87,    3],\n",
              "       [ 187,   13,   29,   18,    2,    1,   88]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vEZyCVylsmU",
        "colab_type": "text"
      },
      "source": [
        "# **Testing new input**\n",
        "Both models were tested and below is testing new input sentences.\n",
        "Different to reference, we need to tokenize it first before implementing evaluated model above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jHPaPI-um8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_emo(model, sentence, min_len = 4):\n",
        "    model.eval()\n",
        "    # 이 부분에서 그냥 바로 tokenizing\n",
        "    tokenized = tokenizer(sentence)\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT_emo.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    preds = model(tensor)\n",
        "    max_preds = preds.argmax(dim = 1)\n",
        "    return max_preds.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3zXVNJ2mK0M",
        "colab_type": "text"
      },
      "source": [
        "Emotion / Utterance classifier will convert digits into char."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVEAG6tt9Req",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emotion_classifier(logits):\n",
        "  global sentiment\n",
        "  if logits == \"0\":\n",
        "    sentiment = '중립'\n",
        "  elif logits == \"1\":\n",
        "    sentiment = '공포'\n",
        "  elif logits == \"2\":\n",
        "    sentiment = '놀람'\n",
        "  elif logits == \"3\":\n",
        "    sentiment = '분노'\n",
        "  elif logits == \"4\":\n",
        "    sentiment = '슬픔'\n",
        "  elif logits == \"5\":\n",
        "    sentiment = '행복'\n",
        "  elif logits == \"6\":\n",
        "    sentiment = '혐오'\n",
        "\n",
        "  return sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbNHks9b9Saj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_utt(model, sentence, min_len = 4):\n",
        "    model.eval()\n",
        "    # 이 부분에서 그냥 바로 tokenizing\n",
        "    tokenized = tokenizer(sentence)\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT_utt.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    preds = model(tensor)\n",
        "    max_preds = preds.argmax(dim = 1)\n",
        "    return max_preds.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSTCj55V9aYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def utterance_classifier(logits):\n",
        "  global utterance\n",
        "  if logits == \"0\":\n",
        "    utterance = '미완'\n",
        "  elif logits == \"1\":\n",
        "    utterance = '서술'\n",
        "  elif logits == \"2\":\n",
        "    utterance = '질문'\n",
        "  elif logits == \"3\":\n",
        "    utterance = '요구'\n",
        "  elif logits == \"4\":\n",
        "    utterance = '수사의문'\n",
        "  elif logits == \"5\":\n",
        "    utterance = '수사명령'\n",
        "  elif logits == \"6\":\n",
        "    utterance = '억양'\n",
        "\n",
        "  return utterance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntt1TnNImSIP",
        "colab_type": "text"
      },
      "source": [
        "Below is tesitng section for new inputs. <br>\n",
        "Enjoy !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lslMJKlj9chh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0658ae4b-9ae4-4d23-9fd9-70bfbd4c60e5"
      },
      "source": [
        "pred_sentence = input()\n",
        "pred_emo = predict_emo(model_emo, pred_sentence)\n",
        "pred_utt = predict_utt(model_utt, pred_sentence)\n",
        "logit_1 = LABEL_emo.vocab.itos[pred_emo]\n",
        "logit_2 = LABEL_utt.vocab.itos[pred_utt]\n",
        "\n",
        "print(f'이 문장의 감정은 {emotion_classifier(logit_1)}이고, 발화 의도는 {utterance_classifier(logit_2)}입니다')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "프로젝트가 끝나서 너무 기쁘지 않니?\n",
            "이 문장의 감정은 행복이고, 발화 의도는 서술입니다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGaSg7Mt4VHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bfa13767-58f3-4b5b-dcd7-29b7c93682f2"
      },
      "source": [
        "pred_sentence = input()\n",
        "pred_emo = predict_emo(model_emo, pred_sentence)\n",
        "pred_utt = predict_utt(model_utt, pred_sentence)\n",
        "logit_1 = LABEL_emo.vocab.itos[pred_emo]\n",
        "logit_2 = LABEL_utt.vocab.itos[pred_utt]\n",
        "\n",
        "print(f'이 문장의 감정은 {emotion_classifier(logit_1)}이고, 발화 의도는 {utterance_classifier(logit_2)}입니다')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "그렇지만 학점이 좋지 않을 텐데.\n",
            "이 문장의 감정은 공포이고, 발화 의도는 서술입니다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezRdknf84W8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2655db70-aa7d-4b09-aea9-af9cde638f01"
      },
      "source": [
        "pred_sentence = input()\n",
        "pred_emo = predict_emo(model_emo, pred_sentence)\n",
        "pred_utt = predict_utt(model_utt, pred_sentence)\n",
        "logit_1 = LABEL_emo.vocab.itos[pred_emo]\n",
        "logit_2 = LABEL_utt.vocab.itos[pred_utt]\n",
        "\n",
        "print(f'이 문장의 감정은 {emotion_classifier(logit_1)}이고, 발화 의도는 {utterance_classifier(logit_2)}입니다')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "서술 말고 다른 걸 말해.\n",
            "이 문장의 감정은 중립이고, 발화 의도는 요구입니다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91vRSvul4dJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}